{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq24UvixibZR"
   },
   "source": [
    "# 'Corona' vor und nach Covid-19 - eine Untersuchung mit Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bedeutungen des Lemmas 'Corona'\n",
    "\n",
    "1. [umgangssprachlich] entsprechend der Bedeutung von Coronavirus <br>\n",
    "2. [metonymisch] durch das Coronavirus hervorgerufene epidemische Infektionskrankheit, durch das Virus verursachte Pandemie, damit verbundene Krise, Maßnahmen des Seuchenschutzes o. Ä. <br>\n",
    "(Quelle: https://www.dwds.de/wb/Corona) <br>\n",
    "<br>\n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Vor der Covid-19-Pandemie wurde der Begriff im Deutschen nur als Eigenname verwendet. Durch die Benennung der Pandemie hat dieses Lemma eine lexikalische Innovation hin zu den oben aufgeführten Bedeutungen durchlaufen. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FQnxIrfibZf"
   },
   "source": [
    "## Importe und Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIZS463EibZh",
    "outputId": "d8d8fe3f-70be-4322-a081-ba1b02ef7f2f"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from joblib import Parallel, delayed  \n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bjlyp7Bjiu60",
    "outputId": "83b51d0b-4222-48c0-ecb3-a6a2d926c0ae"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')\n",
    "stopwords = stopwords.words('german')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Korpus laden, vorbereiten und teilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "QaI8RPMJibZj",
    "outputId": "7a4e7af2-20e5-4048-a2b1-341f8c0a8568"
   },
   "outputs": [],
   "source": [
    "# die Datei enthält alle Treffer des Lemmas 'corona' im ZDL-Regionalkorpus des DWDS \n",
    "# mit jeweils einem Satz Kontext davor und danach\n",
    "\n",
    "df = pd.read_csv('../data/corona_1993-2021.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN durch Whitespace ersetzen\n",
    "\n",
    "df = df.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bevor die Spalten verbunden werden: Whitespace einfügen, um ein Aneinanderkleben der Wörter zu verhindern\n",
    "\n",
    "df['ContextBefore'] = df['ContextBefore'].astype(str) + ' '\n",
    "df['ContextAfter'] = ' ' + df['ContextAfter'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Date</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Bibl</th>\n",
       "      <th>ContextBefore</th>\n",
       "      <th>Hit</th>\n",
       "      <th>ContextAfter</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saar_regional</td>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Saarbrücker Zeitung, 01.02.1993</td>\n",
       "      <td>Filmprogramme Neunkirchen.</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.</td>\n",
       "      <td>Corona 2: \"Whoopi Sister act, eine himmlische...</td>\n",
       "      <td>Filmprogramme Neunkirchen. Corona 1: \"Bodyguar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saar_regional</td>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Saarbrücker Zeitung, 01.02.1993</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.</td>\n",
       "      <td>Corona 2: \"Whoopi Sister act, eine himmlische ...</td>\n",
       "      <td>Burg 1:15.15, 17.45 und 20.15 Uhr \"Der letzte...</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saar_regional</td>\n",
       "      <td>1993-02-03</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Saarbrücker Zeitung, 03.02.1993</td>\n",
       "      <td>Filmprogramme Neunkirchen.</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.</td>\n",
       "      <td>Corona 2: \"Whoopi Sister act, eine himmlische...</td>\n",
       "      <td>Filmprogramme Neunkirchen. Corona 1: \"Bodyguar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saar_regional</td>\n",
       "      <td>1993-02-03</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Saarbrücker Zeitung, 03.02.1993</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.</td>\n",
       "      <td>Corona 2: \"Whoopi Sister act, eine himmlische ...</td>\n",
       "      <td>Burg 1:15.15, 17.45 und 20.15 Uhr \"Der letzte...</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saar_regional</td>\n",
       "      <td>1993-02-04</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Saarbrücker Zeitung, 04.02.1993</td>\n",
       "      <td>Filmprogramme Neunkirchen.</td>\n",
       "      <td>Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.</td>\n",
       "      <td>Corona 2: \"Whoopi Sister act, eine himmlische...</td>\n",
       "      <td>Filmprogramme Neunkirchen. Corona 1: \"Bodyguar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Corpus        Date    Genre                             Bibl  \\\n",
       "0  saar_regional  1993-02-01  Zeitung  Saarbrücker Zeitung, 01.02.1993   \n",
       "1  saar_regional  1993-02-01  Zeitung  Saarbrücker Zeitung, 01.02.1993   \n",
       "2  saar_regional  1993-02-03  Zeitung  Saarbrücker Zeitung, 03.02.1993   \n",
       "3  saar_regional  1993-02-03  Zeitung  Saarbrücker Zeitung, 03.02.1993   \n",
       "4  saar_regional  1993-02-04  Zeitung  Saarbrücker Zeitung, 04.02.1993   \n",
       "\n",
       "                                     ContextBefore  \\\n",
       "0                      Filmprogramme Neunkirchen.    \n",
       "1  Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.    \n",
       "2                      Filmprogramme Neunkirchen.    \n",
       "3  Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.    \n",
       "4                      Filmprogramme Neunkirchen.    \n",
       "\n",
       "                                                 Hit  \\\n",
       "0     Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.   \n",
       "1  Corona 2: \"Whoopi Sister act, eine himmlische ...   \n",
       "2     Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.   \n",
       "3  Corona 2: \"Whoopi Sister act, eine himmlische ...   \n",
       "4     Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr.   \n",
       "\n",
       "                                        ContextAfter  \\\n",
       "0   Corona 2: \"Whoopi Sister act, eine himmlische...   \n",
       "1   Burg 1:15.15, 17.45 und 20.15 Uhr \"Der letzte...   \n",
       "2   Corona 2: \"Whoopi Sister act, eine himmlische...   \n",
       "3   Burg 1:15.15, 17.45 und 20.15 Uhr \"Der letzte...   \n",
       "4   Corona 2: \"Whoopi Sister act, eine himmlische...   \n",
       "\n",
       "                                                Text  \n",
       "0  Filmprogramme Neunkirchen. Corona 1: \"Bodyguar...  \n",
       "1  Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr....  \n",
       "2  Filmprogramme Neunkirchen. Corona 1: \"Bodyguar...  \n",
       "3  Corona 1: \"Bodyguard\" 15.30, 17.45, 20.15 Uhr....  \n",
       "4  Filmprogramme Neunkirchen. Corona 1: \"Bodyguar...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text zu einer Spalte verbinden\n",
    "\n",
    "columns = ['ContextBefore', 'Hit', 'ContextAfter']\n",
    "\n",
    "df['Text'] = df[columns].astype(str).sum(axis=1) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6uioii_SibZl"
   },
   "outputs": [],
   "source": [
    "# Korpus in 2 Teilkorpora splitten: vor und nach Covid-19 (dem Ausbruch in Deutschland)\n",
    "# hier wurde die Grenze zwischen Januar und Februar 2020 gezogen\n",
    "\n",
    "df_before_corona = df.iloc[:9714,:]\n",
    "df_after_corona = df.iloc[9715:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Date</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Bibl</th>\n",
       "      <th>ContextBefore</th>\n",
       "      <th>Hit</th>\n",
       "      <th>ContextAfter</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>tsp_regional</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Der Tagesspiegel, 01.02.2020</td>\n",
       "      <td>Und ich hörte, wie seine Kollegin ihn noch fra...</td>\n",
       "      <td>\"Haste ooch Angst, dich mit Corona anzustecken?\"</td>\n",
       "      <td>Ich habe die beiden nicht konfrontiert.</td>\n",
       "      <td>Und ich hörte, wie seine Kollegin ihn noch fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>ta_regional</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Thüringer Allgemeine, 01.02.2020</td>\n",
       "      <td></td>\n",
       "      <td>Corona: Uni sagt Feier ab</td>\n",
       "      <td>Produkte für chinesisches Neujahrsfest wurden...</td>\n",
       "      <td>Corona: Uni sagt Feier ab Produkte für chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>svz_regional</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Schweriner Volkszeitung, 01.02.2020</td>\n",
       "      <td></td>\n",
       "      <td>Corona: Rostocker erlebt China</td>\n",
       "      <td>Anja Engel</td>\n",
       "      <td>Corona: Rostocker erlebt China Anja Engel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>rztg_regional</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Rhein-Zeitung, 01.02.2020</td>\n",
       "      <td>[...]immungswellen mitreißen lässt, und doch i...</td>\n",
       "      <td>Wenn früher jemand zu mir sagte \" Ich bringe C...</td>\n",
       "      <td>Heute denke ich sofort an Viren, die mich kra...</td>\n",
       "      <td>[...]immungswellen mitreißen lässt, und doch i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>rztg_regional</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Zeitung</td>\n",
       "      <td>Rhein-Zeitung, 01.02.2020</td>\n",
       "      <td>[...]aufland: auch hier ein Teenager, der mit ...</td>\n",
       "      <td>Statt anzunehmen, dass er vielleicht gerade na...</td>\n",
       "      <td>Eigentlich bin ich niemand, der sich von Stim...</td>\n",
       "      <td>[...]aufland: auch hier ein Teenager, der mit ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Corpus        Date    Genre                                 Bibl  \\\n",
       "9715   tsp_regional  2020-02-01  Zeitung         Der Tagesspiegel, 01.02.2020   \n",
       "9716    ta_regional  2020-02-01  Zeitung     Thüringer Allgemeine, 01.02.2020   \n",
       "9717   svz_regional  2020-02-01  Zeitung  Schweriner Volkszeitung, 01.02.2020   \n",
       "9718  rztg_regional  2020-02-01  Zeitung            Rhein-Zeitung, 01.02.2020   \n",
       "9719  rztg_regional  2020-02-01  Zeitung            Rhein-Zeitung, 01.02.2020   \n",
       "\n",
       "                                          ContextBefore  \\\n",
       "9715  Und ich hörte, wie seine Kollegin ihn noch fra...   \n",
       "9716                                                      \n",
       "9717                                                      \n",
       "9718  [...]immungswellen mitreißen lässt, und doch i...   \n",
       "9719  [...]aufland: auch hier ein Teenager, der mit ...   \n",
       "\n",
       "                                                    Hit  \\\n",
       "9715   \"Haste ooch Angst, dich mit Corona anzustecken?\"   \n",
       "9716                          Corona: Uni sagt Feier ab   \n",
       "9717                     Corona: Rostocker erlebt China   \n",
       "9718  Wenn früher jemand zu mir sagte \" Ich bringe C...   \n",
       "9719  Statt anzunehmen, dass er vielleicht gerade na...   \n",
       "\n",
       "                                           ContextAfter  \\\n",
       "9715            Ich habe die beiden nicht konfrontiert.   \n",
       "9716   Produkte für chinesisches Neujahrsfest wurden...   \n",
       "9717                                         Anja Engel   \n",
       "9718   Heute denke ich sofort an Viren, die mich kra...   \n",
       "9719   Eigentlich bin ich niemand, der sich von Stim...   \n",
       "\n",
       "                                                   Text  \n",
       "9715  Und ich hörte, wie seine Kollegin ihn noch fra...  \n",
       "9716    Corona: Uni sagt Feier ab Produkte für chine...  \n",
       "9717          Corona: Rostocker erlebt China Anja Engel  \n",
       "9718  [...]immungswellen mitreißen lässt, und doch i...  \n",
       "9719  [...]aufland: auch hier ein Teenager, der mit ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_corona.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hilfsfunktionen zur Vorbereitung der Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text_column(df, column):\n",
    "    \"\"\"\n",
    "    transforms the Dataframe-column in a lemmatized string\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for i in df[column]:\n",
    "        doc = nlp(i)\n",
    "        lemmas = ' '.join([x.lemma_ for x in doc])\n",
    "        text = text + lemmas\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_to_wordlist(raw:str):\n",
    "    \"\"\"\n",
    "    cleans and tokenizes the sentences\n",
    "    \"\"\"\n",
    "    text = re.sub('[^A-Za-z_äÄöÖüÜß]',' ', raw).split()\n",
    "    filtered_text = [word for word in text if word not in stopwords]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \"\"\"\n",
    "    returns a list of tokenized sentences\n",
    "    \"\"\"\n",
    "    raw_sentences = tokenizer.tokenize(str(raw_text).lower())    \n",
    "    tokenized_sentences = Parallel(n_jobs=-1)(delayed(sentence_to_wordlist)(raw_sentence) for raw_sentence in raw_sentences)\n",
    "    phrases = Phrases(tokenized_sentences)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = list(bigram[tokenized_sentences])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorbereitung des ersten Texts (vor Covid-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uVMUJzlpibZo",
    "outputId": "d9d2a45d-3e51-41e2-83a8-decaed115ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corona', 'whoopi_sister', 'act_himmlisch', 'karriere', 'uhr', 'corona', 'bodyguard', 'uhr']\n"
     ]
    }
   ],
   "source": [
    "text = lemmatize_text_column(df_before_corona, 'Text')\n",
    "\n",
    "sentences = prepare_text(text)\n",
    "\n",
    "# sentences ist eine Liste von tokenisierten Sätzen, zum Beispiel:\n",
    "print(sentences[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorbereitung des zweiten Texts (nach Covid-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aufland', 'teenager', 'mundschutz', 'herumläuft']\n"
     ]
    }
   ],
   "source": [
    "# Achtung, das hier läuft einige Zeit...\n",
    "\n",
    "text2 = lemmatize_text_column(df_after_corona, 'Text')\n",
    "sentences2 = prepare_text(text2)\n",
    "\n",
    "print(sentences2[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training von Word2Vec auf den Text vor Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HI1V5EARibZo"
   },
   "outputs": [],
   "source": [
    "# Paramter setzen\n",
    "workers = 4                      # Use these many worker threads to train the model (=faster training with multicore machines)\n",
    "seed = 42                        # Seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gWLaiZ5cibZq"
   },
   "outputs": [],
   "source": [
    "# Ordner anlegen zum Abspeichern von trainierten Modellen\n",
    "if not os.path.exists('../trained_models'):\n",
    "    os.makedirs('../trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kpbrlbf0ibZq",
    "outputId": "1f537e5c-add4-40c4-8b31-49c6f8869309"
   },
   "outputs": [],
   "source": [
    "w2v_bc = Word2Vec(sentences=sentences,                   \n",
    "                 vector_size=300,          # Dimensionality of the word vectors\n",
    "                 window=10,                # The maximum distance between the current and predicted word within a sentence\n",
    "                 min_count=3,              # (int, optional) – The model ignores all words with total frequency lower than this\n",
    "                 workers=workers, \n",
    "                 min_alpha=0.0001,         # Learning rate will linearly drop to min_alpha as training progresses\n",
    "                 sg=1,                     # Training algorithm: skip-gram if sg=1, otherwise CBOW\n",
    "                 seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH8M_UkAibZr"
   },
   "source": [
    "## Training von Word2Vec auf den Text nach Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Gfy3LgWyibZt",
    "outputId": "0962a7be-cecd-4c1f-a1e0-ba52aff1fdc1"
   },
   "outputs": [],
   "source": [
    "w2v_ac = Word2Vec(sentences=sentences2,                   \n",
    "                 vector_size=300,                \n",
    "                 window=10,              \n",
    "                 min_count=3,             \n",
    "                 workers=workers, \n",
    "                 min_alpha=0.0001,                                                    \n",
    "                 sg=1,                     \n",
    "                 seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainierte Modelle speichern\n",
    "w2v_bc.save(os.path.join('../trained_models', 'w2v_bc_corona.model'))\n",
    "w2v_ac.save(os.path.join('../trained_models', 'w2v_ac_corona.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration und Vergleich der Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainierte Modelle laden\n",
    "w2v_bc = Word2Vec.load(os.path.join('../trained_models', 'w2v_bc_corona.model'))\n",
    "w2v_ac = Word2Vec.load(os.path.join('../trained_models', 'w2v_ac_corona.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "okTISay5ibZv",
    "outputId": "8f2fe288-c8ba-4abc-881f-a2ed7c6e69f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bram_stoker', 0.7868413329124451),\n",
       " ('cinetower', 0.7812503576278687),\n",
       " ('van_damm', 0.7658439874649048),\n",
       " ('pron_dracula', 0.7546699047088623),\n",
       " ('s_dracula', 0.7217091917991638),\n",
       " ('costner_perfect', 0.7090845108032227),\n",
       " ('ausweg', 0.7064926028251648),\n",
       " ('harte_ziel', 0.7055076360702515),\n",
       " ('world', 0.6913805603981018),\n",
       " ('mission_moscou', 0.6840837597846985),\n",
       " ('lesen_wahr', 0.6817106604576111),\n",
       " ('act_himmlisch', 0.6804236173629761),\n",
       " ('göttlich_mission', 0.672395646572113),\n",
       " ('bodyguard', 0.6713323593139648),\n",
       " ('karriere', 0.6709579229354858),\n",
       " ('whoopi_sister', 0.6687706112861633),\n",
       " ('alarmstufe_rot', 0.6637182831764221),\n",
       " ('verrückt_tradition', 0.6604779958724976),\n",
       " ('schwarzenegger_true', 0.6584728956222534),\n",
       " ('made_of', 0.6572145223617554),\n",
       " ('uhr', 0.6547415852546692),\n",
       " ('wayne_s', 0.6535637378692627),\n",
       " ('addams_family', 0.6511383056640625),\n",
       " ('sister_act', 0.6490532755851746),\n",
       " ('samstag', 0.6481463313102722)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ähnliche Wörter zu 'corona' vor Covid-19\n",
    "w2v_bc.wv.most_similar(positive=['corona'], topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "twofLe79ibZw",
    "outputId": "baea5bc0-fcdb-4e83-c22c-37853c505e65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asylsuchende', 0.5901619791984558),\n",
       " ('hochzeitstermine', 0.5898657441139221),\n",
       " ('stichtag', 0.5832740068435669),\n",
       " ('weitder', 0.5788891315460205),\n",
       " ('verlautbarung', 0.5786030292510986),\n",
       " ('schutzmaske_tragen', 0.5709327459335327),\n",
       " ('ged', 0.5669485330581665),\n",
       " ('früherkennung', 0.5646762847900391),\n",
       " ('religionspädagogik', 0.560499370098114),\n",
       " ('gi', 0.559234082698822),\n",
       " ('dreistellig', 0.5571960210800171),\n",
       " ('lage_entspannen', 0.5567444562911987),\n",
       " ('normalisierung', 0.5567006468772888),\n",
       " ('nachfolgend', 0.5557396411895752),\n",
       " ('thumanns', 0.554460346698761),\n",
       " ('verbunden_kontaktbeschränkungen', 0.5542062520980835),\n",
       " ('sorg', 0.5540571808815002),\n",
       " ('hundertprozentig', 0.5538737177848816),\n",
       " ('statisch', 0.5535236597061157),\n",
       " ('regelbetriebs', 0.5529605150222778),\n",
       " ('vertagung', 0.5523310899734497),\n",
       " ('mahnung', 0.5522242188453674),\n",
       " ('bernkastel_wittlich', 0.5515028834342957),\n",
       " ('woran_liegen', 0.5511519312858582),\n",
       " ('frühling_sommer', 0.5510959029197693)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ähnliche Wörter zu 'corona' nach Covid-19\n",
    "w2v_ac.wv.most_similar(positive=['corona'], topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coronavirus', 0.5538238286972046),\n",
       " ('ausbreitung', 0.5424193143844604),\n",
       " ('sars_cov', 0.5320586562156677),\n",
       " ('befallen', 0.5139110088348389),\n",
       " ('entwickler_ih', 0.5116097331047058),\n",
       " ('bamberger_computer', 0.5101991295814514),\n",
       " ('epidemie', 0.5093218684196472),\n",
       " ('krise_leisten', 0.503666341304779),\n",
       " ('erreger', 0.49772801995277405),\n",
       " ('covid', 0.4925921857357025)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_ac.wv.most_similar(positive=['virus'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weiteres Vorgehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimierung der Hyperparameter von Word2Vec?\n",
    "- Ausrichtung der beiden Embedding-Modelle, um Vergleichbarkeit zu schaffen (d.h. die gleiche Art von semantischen Informationen müssen in übereinstimmenden Dimensionen codiert sein)\n",
    "<br> --> Versuch mit Orthogonal Procrustes \n",
    "- Vergleich der Vektoren (Cosinus-Similarität bzw. -Abstand)\n",
    "- Überprüfung anhand anderer Vektoren und durch Samplen der Korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausrichtung der beiden Embedding-Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: https://gist.github.com/zhicongchen/9e23d5c3f1e5b1293b16133485cd17d8\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Updatet script: https://gist.github.com/zhicongchen/9e23d5c3f1e5b1293b16133485cd17d8\n",
    "    Procrustes align two gensim models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "    \n",
    "    # re-filling the normed vectors\n",
    "    in_base_embed.wv.fill_norms(force=True)\n",
    "    in_other_embed.wv.fill_norms(force=True)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed\n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4824 4824\n",
      "4824 4824\n"
     ]
    }
   ],
   "source": [
    "# Ausrichtung des \"After-Corona-Modells\" an das \"Before-Corona-Modell\"\n",
    "\n",
    "w2v_ac_al = smart_procrustes_align_gensim(w2v_bc, w2v_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speichern\n",
    "w2v_ac_al.save(os.path.join('../trained_models', 'w2v_ac_al_corona.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laden\n",
    "w2v_ac_al = Word2Vec.load(os.path.join('../trained_models', 'w2v_ac_al_corona.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('umlauf', 0.5463956594467163),\n",
       " ('beka', 0.5388159155845642),\n",
       " ('ve', 0.5376611351966858),\n",
       " ('kreis_neuwied', 0.525550365447998),\n",
       " ('konzertiert', 0.5225399136543274),\n",
       " ('zehnte', 0.5222098231315613),\n",
       " ('psycho', 0.5221653580665588),\n",
       " ('reges', 0.5194510221481323),\n",
       " ('arzbach', 0.519038200378418),\n",
       " ('schwert', 0.5138117671012878),\n",
       " ('edward', 0.5132358074188232),\n",
       " ('bergrettung', 0.513198733329773),\n",
       " ('pils', 0.5094652771949768),\n",
       " ('abendlich', 0.509449303150177),\n",
       " ('skif', 0.5090993642807007),\n",
       " ('nunmehr', 0.5086882710456848),\n",
       " ('epoche', 0.5078120231628418),\n",
       " ('kursus', 0.5074869394302368),\n",
       " ('schuldig', 0.5067552328109741),\n",
       " ('explosiv', 0.5067422986030579),\n",
       " ('unendliche', 0.5057722926139832),\n",
       " ('kirchgang', 0.5057617425918579),\n",
       " ('illumination', 0.5053489804267883),\n",
       " ('tägl', 0.5050823092460632),\n",
       " ('prignitz', 0.5047396421432495)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ähnliche Wörter zu 'corona' nach Covid-19\n",
    "w2v_ac_al.wv.most_similar(positive=['corona'], topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5456673502922058"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_corona_bc = w2v_bc.wv['corona']  \n",
    "vector_corona_ac_al = w2v_ac_al.wv['corona'] \n",
    "\n",
    "cosine_corona = 1 - spatial.distance.cosine(vector_corona_bc, vector_corona_ac_al)\n",
    "cosine_corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49820369482040405"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_bericht_bc = w2v_bc.wv['bericht']  \n",
    "vector_bericht_ac_al = w2v_ac_al.wv['bericht'] \n",
    "\n",
    "cosine_bericht = 1 - spatial.distance.cosine(vector_bericht_bc, vector_bericht_ac_al)\n",
    "cosine_bericht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48358750343322754"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_deutschland_bc = w2v_bc.wv['deutschland']  \n",
    "vector_deutschland_ac_al = w2v_ac_al.wv['deutschland'] \n",
    "\n",
    "cosine_deutschland = 1 - spatial.distance.cosine(vector_deutschland_bc, vector_deutschland_ac_al)\n",
    "cosine_deutschland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36245864629745483"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_nachricht_bc = w2v_bc.wv['nachricht']  \n",
    "vector_nachricht_ac_al = w2v_ac_al.wv['nachricht'] \n",
    "\n",
    "cosine_nachricht = 1 - spatial.distance.cosine(vector_nachricht_bc, vector_nachricht_ac_al)\n",
    "cosine_nachricht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3227536976337433"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_virus_bc = w2v_bc.wv['virus']  \n",
    "vector_virus_ac_al = w2v_ac_al.wv['virus'] \n",
    "\n",
    "cosine_virus = 1 - spatial.distance.cosine(vector_virus_bc, vector_virus_ac_al)\n",
    "cosine_virus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiteres Vorgehen \n",
    "- von allen Wörtern/Vektoren jeweils Cosinus-Ähnlichkeit berechnen\n",
    "- durchschnittliche Ähnlichkeit und Standardfehler\n",
    "- ist die Ähnlichkeit von \"Kontaktverbot\" vorher und nachher innerhalb des Standardfehlers?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fasttext Corona.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
